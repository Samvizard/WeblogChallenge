

from pyspark.sql import SparkSession
from pyspark.sql import functions as f
from pyspark.sql.window import Window

# spark = SparkSession\
#     .builder\
#     .master("local")\
#     .appName("weblogCompetition")\
#     .getOrCreate()

# preliminary Step is used to unzip log.gz file to .log file using gunzip
# Step 1: Read log file using spark csv api with delimiter space
raw_df = spark \
            .read \
            .option("delimiter", " ") \
            .csv("data/2015_07_22_mktplace_shop_web_log_sample.log")


# Step 2: Get Ipaddr: from first item from _c2
#        Get URL: from second item from column _c11

filter_log_df = raw_df \
            .select("_c0","_c2","_c11") \
            .withColumn("urlAll", f.split("_c11", " ")) \
                 .withColumn("url",f.trim(f.col('urlAll').getItem(1))) \
            .withColumn("ipaddrAll", f.split("_c2", ":")) \
                .withColumn("ipAddr",f.trim(f.col('ipaddrAll').getItem(0))) \
            .drop("_c2","_c11","urlAll","ipaddrAll")


# Step 3: Cast Timestamp column
format = "yyyy-MM-dd'T'HH:mm:ss.SSSSSSZ"
log_df = filter_log_df \
            .withColumn("timestamp",(f.col("_c0").cast('timestamp'))) \
            .drop("_c0")

# Step 4 : Cache the final dataframe
log_df.cache()
log_df.count()


# Step 5 : find Lag on timestamp over partitions generated by the window function.
# Each partition is generated from the window function grouping ipAddr
# with ascending timestamp.
# new column: prev_timestamp with lag is added

w = Window \
         .partitionBy("ipAddr") \
         .orderBy("timestamp")

log_with_lag = log_df.withColumn("prev_timestamp", f.lag("timestamp").over(w))
log_with_lag.orderBy("ipAddr","timestamp").show(10,False)


# Step 6 : Convert the time stamp string into UTC elapsed
#seconds to compute the lagged time per min unit
lag_in_min = log_with_lag \
                .withColumn('lag_in_min', \
                 (f.unix_timestamp('timestamp') - f.unix_timestamp('prev_timestamp'))/60)



# Step 7 : If the time gap between two events is
#greater than 30 mins, mark it as a new session.

new_session = lag_in_min \
                .withColumn('is_new_session', \
                 f.when( f.col('lag_in_min') > 30 , 1).otherwise(0))


# Step 8 :  Sum ‘is_new_session’ over partitions generated by the window function.
# Each partition is generated from the window function grouping user_id with ascending timestamp.
# Then the summed result could be used as a session_id within every user’s behavior events.
w2 = Window \
        .partitionBy('ipAddr') \
        .orderBy('timestamp')

user_session = new_session \
         .withColumn("session_id", f.sum('is_new_session').over(w2))

user_session.cache()


# Step 8 : Get Each Session time as well as min_lag, max_lag,
# avg_lag, min_time, max_lag
user_session_time = user_session \
     .groupBy("ipAddr","session_id") \
     .agg(f.min("timestamp").alias("min_time"), \
            f.max("timestamp").alias("max_time"), \
            f.avg("lag_in_min").alias("avg_lag"), \
            f.min("lag_in_min").alias("min_lag"), \
            f.max("lag_in_min").alias("max_lag"), \
            f.count("ipAddr")) \
    .withColumn('session_time', \
        (f.unix_timestamp('max_time') - f.unix_timestamp('min_time'))/60)


# Step 8 :  Q-1 :Determine the average session time

user_session_time \
         .select("ipAddr","session_id",
            "session_time","max_lag","min_time","max_time")\
         .orderBy(f.desc("session_time")) \
         .show(5)

user_session_time \
         .groupBy("ipAddr")\
         .agg(f.avg("session_time").alias("avg_session_time"))\
         .orderBy(f.desc("avg_session_time"))\
         .show(10)


# #### Q-2 : Determine unique URL visits per session. To clarify, count a hit to a unique URL only once per session.

# unique URL visits per session per user
user_unique_urls = user_session \
                        .select("ipAddr","session_id","url") \
                        .distinct()

user_unique_urls.show(5,False)
user_unique_urls.count()


# #### Q-3 :Find the most engaged users, ie the IPs with the longest session times
# considering longest session time more than 30 mins
user_session_time \
         .filter("session_time > 30") \
         .select("ipAddr") \
         .distinct() \
         .count()

#Note:  The above computed Dataframes can be stored into a persistent storage such as Hive, Hbase, HDFS etc.
